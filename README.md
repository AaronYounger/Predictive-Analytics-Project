# Overview of Predictive Analytics Project
> This project serves as a demonstration of applied knowledge in key predictive analytics concepts and techniques. This project consists of three separate reports, each report tied to a unique dataset. To meet the project criteria, each dataset was required to be publicly available, including a data dictionary or metadata, support the formulation of an interesting and relevant business problem, and originate from a real, non-synthetic source. Each report was structured using the phases of the CRISP-DM Methodology. The CRISP-DM methodology is widely used for data science and data mining projects as it provides a structured, flexible, business-focused framework that aligns analytical work back to the business problem. The six phases of the CRISP-DM Methodology include Business Understanding, Data Understanding, Data Preparation, Model Development, Model Evaluation, and Deployment. Alongside the three reports, ethical considerations were explored, focusing on data collection and storage practices within the relevant sectors of the data sets and the ethical principles outlined in the Data Science Code of Conduct. This project was developed using R and the IDE RStudio.
     
> To present the three reports, this repository includes three Quarto (.qmd) files containing the R code and three corresponding PDF files that display the rendered output. Each report focuses on a single modeling objective. One report will utilize classification models to answer a categorical/binary business problem, one report will just utilize regression-based models to answer a continuous/numeric business problem, and the final report will utilize time-series models to predict future observations. Below is a brief explanation of what I did for each of these reports.

## Overview of Classification Report
> The Classification report followed the six phases of the CRISP-DM methodology starting with Business Understanding. The Business Problem Statement that the analysis set out to answer was how to identify which clients are more likely to subscribe to a term deposit. The dataset used for this classification report originated from a bank. Banks benefit from predicting which clients are most likely to subscribe to term deposits because it enables more targeted and effective marketing, reducing marketing costs while allowing banks to focus on customer segments with higher subscription likelihood, increasing revenue. In the Data Understanding phase EDA was split into categorical and Numeric EDA. My categorical EDA found issues with level and class imbalance while my numeric EDA found issues with distribution and correlation. For my Data Preparation phase I removed extreme outliers to help with variable distribution and partitioned my data for modeling. The models explored in this project include classification trees, k-nearest neighbors (KNN), and logistic regression, each of which required distinct data preparation and dataset partitioning approaches. In the modeling phase, the model that was explored first was classification trees. An unweighted and weighted classification tree was made with the weighted classification tree being the best model as it had the best discriminatory power between the two classes of the dependent variable. The second set of models consisted of k-nearest neighbors (KNN) models, including unweighted, oversampled, and threshold-tuned variants. Among these, the threshold-tuned KNN model performed best, demonstrating the strongest discriminatory power between the two classes of the dependent variable. The last set of models consisted of an unweighted and weighted Logistic Regression model. The weighted logistic regression model was the best as it had the strongest discriminatory power between the two classes of the dependent variable. In the evaluation phase, the top-performing models were compared to determine which would be selected for deployment to address the business problem. Among the weighted classification tree, threshold-tuned KNN model, and weighted logistic regression, the weighted logistic regression models the best performer, achieving the highest balanced accuracy and demonstrating strong discriminatory power through a well-balanced sensitivity and specificity. In the deployment phase, the business problem was addressed using the weighted logistic regression model. The weighted logistic regression model serves as a practical tool for banks to identify clients most likely to subscribe to a term deposit, enabling more targeted marketing efforts that reduce costs and increase revenue.

For readers interested in exploring this report in greater depth, please refer to the corresponding Quarto (.qmd) or PDF file for this report.
     
[ðŸ“„ View Classification PDF](Classification-Report/Classification%20Report.pdf)Â·
[ðŸ’» View Classification Code (.qmd)](Classification-Report/Classification%20Project%20Code.qmd).

## Overview of Prediction Report
> The Prediction report followed the six phases of the Crisp-DM methodology. The Business Problem Statement that the analysis set out to answer was to build a predictive model capable of predicting final grades of students. The dataset used for this prediction report originated from a school. An accurate predictive model can benefit schools by identifying the factors that most influence final grades while also enabling early identification of students who may be struggling academically. For the data understanding phase, EDA was split into numeric and categorical EDA. For numeric EDA, certain numeric variables were grouped into binary or multi-leveled categorical variables to help mitigate level imbalance. Distribution, outliers, and correlation was then checked for the remaining numeric variables. For Categorical EDA class and level imbalance was further investigated and a simple random forest model was also made to check variable importance of the categorical variables in predicting the dependent variable. In the data preparation phase, outliers were removed to help with variable distribution. A Lasso Model was used to examine potential interaction effects among predictors, which is useful in linear regression modeling. Lastly, the dataset was partitioned to support the development of regression tree, random forest, and linear regression models. During the Modeling phase, several regression tree specifications were evaluated, including a default tree, a pruned tree, and a more complex tree. Among these models, the complex tree produced the most accurate predictions. The complex tree was constructed by selecting the smallest complexity parameter (cp) value that remained statistically comparable to the optimal pruned tree, which was determined using a calculated threshold value.  The next model developed was a random forest model. This model will be used to evaluate against the other best models. Lastly, a default and refined linear regression model was made. The refined linear regression model only included the statistically significant variables found in the default linear regression model. The best linear regression model was the refined linear regression model. In the evaluation phase, the top-performing models were compared to determine which would be selected for deployment to address the business problem. Among the complex regression tree, random forest, and refined linear regression model, the refined linear regression model was the best performer producing the most accurate predictions. In the deployment phase, the business problem was addressed using the linear regression model. Based on the modeling and evaluation phases, the refined linear regression model was selected, as its predictions were consistently within one grade point of studentsâ€™ final scores. This model provides schools with a practical tool for predicting final grades and identifying students who may require additional academic support.
     
For readers interested in exploring this report in greater depth, please refer to the corresponding Quarto (.qmd) or PDF file for this report. 

[ðŸ“„ View Prediction PDF](Prediction-Report/Prediction_Report.pdf).
[ðŸ’» View Prediction Code (.qmd)](Prediction-Report/Prediction_Project_Code).

## Overview of Forecast Report
> The Forecast report followed the six phases of the CRISP-DM methodology. The Business Problem Statement that this analysis followed was to build a forecast model capable of accurately projecting six months into the future for retail stakeholders. In the Data Understanding phase, the time series dataset was determined to have both trend and seasonality. Five structure breaks were also identified in the time-series dataset. In the Data Preparation stage, both regression and Holtâ€“Winters forecasting models were developed. The time-series data was partitioned into training and testing sets, and for the regression model, two dummy variables were introduced to capture the effects of the two most significant structural breaks. A three-year rolling window validation procedure was conducted on the regression training data, resulting in a final model that incorporated trend, seasonality, and both dummy variables. For the Holtâ€“Winters models, rolling window validation was used to compare AAA and AAM specifications. Along with this the Holt-Winters models were tested on a full dataset and a dataset that just contained data after the most recent structural break to see which ones produced the most accurate results. It was determined that an AAM Holt-Winters model trained on the full dataset produced the best results. During the Modeling phase, the selected best regression and Holtâ€“Winters model was applied to the test dataset to generate forecasted values and evaluation metrics. In the Evaluation phase, these metrics were compared across the two best models to determine which model performed best in forecasting the time-series data. The AAM Holt-Winters Model produced the most accurate forecasting results. In the deployment phase, the AAM Holt-Winters model was used to accurately forecast six months into the future. The AAM Holt-Winters model can be used by retail stakeholders to make accurate predictions for the future. 

For readers interested in exploring this report in greater depth, please refer to the corresponding Quarto (.qmd) or PDF file for this report. 

[ðŸ“„ View Forecast PDF](Forecast-Report/Forecast_Report.pdf).
[ðŸ’» View Forecast Code (.qmd)](Forecast-Report/Forecasting_Report_Code).

## Personal Takeaways From This Project

> The completion of this project required over 100 hours of work. Throughout the process, I encountered many challenges commonly faced by data analysts and data scientists, as well as unique issues specific to the datasets used. From this project I learned many things and would like to highlight a few. First, an analysis should always be geared towards answering the business problem. Although there are many insightful things to be found in data, if it is not helpful to answer the business problem, then it is ultimately useless. Second, take the time to get familiar with the variables in the data set. Many models require data transformations, and without a clear understanding of what each variable represents, misinterpretations can occur that may lead to serious analytical errors. Finally, it is important to be open-minded and committed to continual learning. While each report identified a best-performing model, a model that performs well on one dataset may not be optimal for another. Similarly, models not used in this project could have proved to be the superior model. In the rapidly growing field of data science, ongoing learning and mastery of both new and established techniques is a must, as new methods can provide new and helpful ways to look at data.

